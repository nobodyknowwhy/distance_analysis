海警任务：
0. 代码整体运行逻辑：
①监听文件夹，设置lock和loct
②循环遍历整个数据文件夹，如果勾选is_process_batch=True，则会将所有同一类型的数据合并处理（即所有DM一起处理，所有GK一起处理...）
③在处理字段映射的时候，会正则匹配里面如cb.txt的占用字段和具体内容，如（char sZM[9];    //  站号 char sWD[6];    //  纬度）->（{'站号':9, '维度':6}）
④总体逻辑是会对文件的内容进行字典对应value的切片（中文占两个字符空间），最终处理成df格式数据，并入库，字段的映射也在base.base里面了（map_from_ce），报不保存成df也修改一下参数就行（is_save_df）
⑤经纬度会进行/100的操作，其他异常数据也做了对应处理，具体的处理代码都在process_hj.modify_and_insert_df里面

1. 需要注意的是，由于完全一样的字段无法插入，所以为了确保文件入库的df行字段的值不完全一致，设置了去重操作，在store_data_base.py的insert_to_pg_df里面。
（由于可能设置了一个一个文件的处理，所以可能会有以下情况：一个文件的数据在第二个文件重复出现，导致第二个文件的数据都无法插入，这时候可以修改base.base里面的is_insert_batch，修改成False
   让其原子性变成单个数据而非整个文件)

2. 不能重复插入字段！如果之前的字段已经存在，必须先删除再插入，除非设置原子性是单个数据项而非整个文件，否则整个文件无法处理成功（字段的id设置为：所有值组成一个list再str转化的md5值）

3. 详细的代码运用说明在base.base里面全部有注释

4. 一些可能的疑问：
①为什么不一直设置原子性操作为数据项而非整个文件？     因为效率实在是太低了，插入速率大幅度下降，涉及大量的数据库插入操作，如果想改善的话可以异步进行，或者自行设置批次插入的变量
②那个生成png和json的代码插入在哪里？    插入在code_haijing.process_hj的302行（已经注释），以及322行（还未写），但是程序运行会有一定报错（可以生成json和png，但是不确定生成的是否正确，且无法全部生成），需要排查
③需要什么额外的库？      除了python11环境和对应的包，还需要公司自研的geonut
④如果我要添加多一点的字段处理函数，应该怎么做？      需要修改code_haijing/process_hj.py的modify_and_insert_df，将字段apply函数process_exp即可，对应的process_exp也可以进函数修改






核电任务：
0. 代码整体运行逻辑：
①api接口只会读入json第一部分的时间信息（比如传入顺序3.12，3.13，3.14，会默认读入3.12，并默认下面的几个数据的时间连续，返回3.12三天后开始的连续三天信息）
②构建对应的数据结构（具体为df类型数据，会先进行列平均进行插值操作）给陈昱俊的模型让他进行识别得出结果。
③返回识别结果

1. 一些可能的疑问：
①传入的数据都会用到吗？   并不是，其实只需要20号声呐的数据，所有构建df的时候也只用了20号声呐的数据，如果要改的话，修改bluepoint/api/job.py的18行，将sonar_id改成对应的id就行
②时间能够随意调整吗，如果我现在要求输入前10天预测后3天的数据怎么办？     并不能随便调整，但是如果只调整时间，输入的json也做对应修改，模型也做了适配的话，可以尝试改一下bluepoint/api/job.py第39行的循环逻辑






养殖区任务（具体能够运行的代码在机器841 383 869的容器ccc内，或者D:\hqxkj\wushi2024009F\01code\mdp_remote_sensing\task\process_haizao）
0. 代码整体的运行逻辑：
①从souce表里面监听入库的数据进行解析（按文件大小从小到大返回），读入无法解析的数据gid，直接跳过处理（ERROR_GID.txt）
②剔除数据库中的小图测试数据（valid_gid），过滤掉非tif数据
③遍历每个文件，进入task/process_haizao/alg_breed.py的breed_zone_identification函数进行解析，不能读入解析的数据会被记录下ERROR_GID.txt当中
④解析会进入task/process_haizao/seg的code进行处理，分别执行to_img（将大图片切割成小图片），to_seg（将每个小图片拉伸识别），to_shp（将所有识别结果合并保存）
⑤入库

1. 如果需要保存对应的文件夹结果（shp文件），就在config/scheduler_task.yaml里面save_out_put选择True，可以自定义保存结果/mdp_remote_sensing/data/archive/yzq_result

2. 一些可能的疑问：
①为什么我的代码在容器里面能够运行，但是出了容器就运行不了了，显示路径找不到？    很可能是windows的长路径适应问题导致路径找不到，linux里面没有这个问题，windows需要做长路径适配（'//?/'）
②所有代码都有用吗？          实则不然，其中task里面只有process_haizao是养殖区的任务，其他诸如haizao_remote，nc_h5_monitor，以及process_remote_sensing_data对于养殖区任务来说都是没有用的，当然，如果监听入库工作也算的话nc_h5_monitor还是得保留的
③为什么需要设定ERROR_GID，如何去除这些排除项？      因为如果不进行记录，每一次程序重复运行的时候都会进行一次判断，可能会导致占用内存不必要的增长，无法处理更大的数据，去除只能进入到ccc容器里面进行去除，ERROR_GID.txt会保存在mdp_remote_sensing的工作目录下
④如何让其能够运行更大的数据？       一般的方法是修改task/process_haizao/seg/code/to_img.py里面的clip_raster_by_shapefile函数，将tile_size设置小一点，需要注意的是，越小虽然能处理越大的数据，但是速度越慢






宝安任务：
0. 代码整体运行逻辑：
①查看站点模型存不存在（模型文件名匹配），不存在直接返回不存在
②模型识别得到啊对应结果
③返回

1. 如果需要添加对应的站点，只需要：① 将模型文件放入data/models里面，并进行对应重命名  ②修改config/models.yaml，添加对应的站点模型  ③更改服务器/vdb/hqxsoft/hqxs2023005/02services/baoan_reg，传入model和yaml  ④重启容器docker restart 0bf

2.可能的疑问：
①好像没什么可能的疑问，这个项目问题不算很大，除了一些无用代码







藻类任务（D:\hqxkj\WSXS2024006F\01code\model-seaweed）：
0.整体api运行逻辑：
①等待后端传入数据
②解压文件，并对文件进行解析，如果是tif->png任务，则直接进行对应的解析
③将所有的shp文件通过geopandas合并成df，并将解析的df数据入库

1. 需要注意的是，藻类任务有两个，一个是打捞分区，云图分区的解析，另一个是tif->png的数据解析

3. 可能的一些疑问：
①为啥代码这么多有的没的？      如果是gitlab上面的话代码比较干净，如果是我这台机器，一部分是用来做mdp_base处理的代码，所以看起来乱一点，如果要拿到比较干净的代码可能只能找张增辉去容器里面cp了
②为啥你的运行逻辑写的这么潦草？        代码太过于久远了，忘记了，有不懂的看代码吧




关于一些已经结束的任务，如海底有览模型，wvp摄像头的没什么问题了已经，就也不记录了
关于其他如象山的入库任务，海南的文字识别，语音识别相关的，大语言模型相关的与我关联度较低或者任务等级一般的任务就不记录了